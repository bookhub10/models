#import MetaTrader5 as mt5
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pytz
import talib
import pickle
import os
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import class_weight
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import GRU, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf
import talib

# ==============================================================================
# PART 1: DATA COLLECTION AND PREPROCESSING
# ==============================================================================

# def initialize_mt5(login=0, password="", server=""):
#     """Initialize connection to MetaTrader 5"""
#     if not mt5.initialize():
#         print("initialize() failed, retrying...")
#         if not mt5.initialize():
#             print("initialize() failed finally.")
#             return False
    
#     if login != 0:
#         if not mt5.login(login, password=password, server=server):
#             print("Failed to connect to trade account", login)
#             mt5.shutdown()
#             return False
#     return True

# def get_xauusd_data(days=180, timeframe=mt5.TIMEFRAME_M5):
#     """Get XAUUSD data FOR M5 (Base)"""
#     # (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£)
#     timezone = pytz.timezone("Etc/UTC")
#     time_from = datetime.now(timezone) - timedelta(days=days)
#     rates = mt5.copy_rates_from(
#         "XAUUSD", 
#         timeframe, 
#         time_from, 
#         30000 
#     )
#     if rates is None or len(rates) == 0:
#         print("Failed to get XAUUSD data.")
#         return pd.DataFrame()
#     df = pd.DataFrame(rates)
#     df['time'] = pd.to_datetime(df['time'], unit='s')
#     df.set_index('time', inplace=True)
#     df.drop(columns=['spread', 'real_volume'], inplace=True)
#     return df

# def get_other_timeframe_data(days, timeframe):
#     """üÜï ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TF ‡∏≠‡∏∑‡πà‡∏ô"""
#     timezone = pytz.timezone("Etc/UTC")
#     time_from = datetime.now(timezone) - timedelta(days=days)
#     rates = mt5.copy_rates_from("XAUUSD", timeframe, time_from, 30000)
#     if rates is None or len(rates) == 0:
#         return pd.DataFrame()
#     df = pd.DataFrame(rates)
#     df['time'] = pd.to_datetime(df['time'], unit='s')
#     df.set_index('time', inplace=True)
#     df = df[['close']] # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Ñ‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î
#     return df

# üõë A. ADD TECHNICAL INDICATORS (‡∏ï‡∏≤‡∏° Obot_model) üõë
# üõë [‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ] ‡πÉ‡∏ô model.py (Windows) üõë

def add_technical_indicators(df_m5, df_m30, df_h1):
    """
    ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å M5, M30, H1 ‡πÅ‡∏•‡∏∞ ATR (12 Features)
    """
    
    # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå M5 ===
    df_m5 = df_m5.copy()
    close_prices_m5 = df_m5['close'].values.astype(np.float64)
    high_prices_m5 = df_m5['high'].values.astype(np.float64) # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏°
    low_prices_m5 = df_m5['low'].values.astype(np.float64)   # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏°
    
    df_m5['SMA_10'] = talib.SMA(close_prices_m5, timeperiod=10)
    df_m5['SMA_50'] = talib.SMA(close_prices_m5, timeperiod=50)
    df_m5['Momentum_1'] = talib.MOM(close_prices_m5, timeperiod=1)
    df_m5['High_Low'] = df_m5['high'] - df_m5['low']
    
    # --- ‚¨áÔ∏è 1. [‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà] ‚¨áÔ∏è ---
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ATR (Average True Range) ‡∏ö‡∏ô M5
    df_m5['ATR_14'] = talib.ATR(high_prices_m5, low_prices_m5, close_prices_m5, timeperiod=14)
    # --- ‚¨ÜÔ∏è [‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà] ‚¨ÜÔ∏è ---

    # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå Multi-Timeframe (using TA-Lib) ===
    close_prices_m30 = df_m30['close'].values.astype(np.float64)
    df_m30['M30_RSI'] = talib.RSI(close_prices_m30, timeperiod=14)
    
    close_prices_h1 = df_h1['close'].values.astype(np.float64)
    df_h1['H1_MA_200'] = talib.SMA(close_prices_h1, timeperiod=200)
    df_h1['H1_MA_Trend'] = np.where(df_h1['close'] > df_h1['H1_MA_200'], 1, 0)
    
    # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á" (Alignment) ===
    # (‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° 100%)
    print("Aligning M30 features to M5 timeline...")
    df_combined = pd.merge_asof(
        df_m5.sort_index(), 
        df_m30[['M30_RSI']].sort_index(), 
        left_index=True, 
        right_index=True, 
        direction='backward'
    )
    print("Aligning H1 features to M5 timeline...")
    df_final = pd.merge_asof(
        df_combined.sort_index(),
        df_h1[['H1_MA_Trend']].sort_index(),
        left_index=True,
        right_index=True,
        direction='backward'
    )

    # Feature 13 & 14: RSI Zones (‡πÉ‡∏ä‡πâ M30_RSI ‡∏ó‡∏µ‡πà merge ‡πÅ‡∏•‡πâ‡∏ß)
    df_final['RSI_Overbought'] = np.where(df_final['M30_RSI'] > 70, 1, 0)
    df_final['RSI_Oversold'] = np.where(df_final['M30_RSI'] < 30, 1, 0)
    
    # Feature 15: SMA Crossover (‡πÉ‡∏ä‡πâ SMA_10 ‡πÅ‡∏•‡∏∞ SMA_50 ‡∏ó‡∏µ‡πà merge ‡πÅ‡∏•‡πâ‡∏ß)
    df_final['SMA_Cross'] = np.where(df_final['SMA_10'] > df_final['SMA_50'], 1, 0)
    
    # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• ===
    df_final.dropna(inplace=True)
    df_final.reset_index(drop=True, inplace=True)

    # --- ‚¨áÔ∏è 3. [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‚¨áÔ∏è ---
    # üõë ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (12 ‡πÄ‡∏î‡∏¥‡∏° + 3 ‡πÉ‡∏´‡∏°‡πà = 15 ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå)
    feature_cols = [
        'open', 'high', 'low', 'close', 'tick_volume', 
        'SMA_10', 'SMA_50', 'Momentum_1', 'High_Low',
        'M30_RSI', 'H1_MA_Trend',
        'ATR_14',
        'RSI_Overbought', # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏°
        'RSI_Oversold',   # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏°
        'SMA_Cross'       # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏°
    ]
    # --- ‚¨ÜÔ∏è [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‚¨ÜÔ∏è ---
    
    final_cols = [col for col in feature_cols if col in df_final.columns]
    if len(final_cols) != len(feature_cols):
        print(f"Warning: Missing columns! Expected {len(feature_cols)}, found {len(final_cols)}")
        
    df_final = df_final[final_cols].copy()
    
    print(f"Total features created: {len(df_final.columns)}")
    return df_final

# üõë B. CREATE SEQUENCES AND LABELS (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î) üõë
# --- üõë [‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ] (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô 3-Class Fix) üõë ---

def create_sequences_and_labels(df_features_unscaled, sequence_length=100, lookahead_bars=1, hold_threshold_pct=0.0005):
    """
    ‡∏£‡∏±‡∏ö Unscaled Features (12 cols), ‡∏™‡∏£‡πâ‡∏≤‡∏á Unscaled X (3D) ‡πÅ‡∏•‡∏∞ Labels y (1D)
    """
    X, y = [], []
    
    # 1. üõë [FIX] ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å DF ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Warning
    df = df_features_unscaled.copy() 
    
    # 2. üõë [FIX] ‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∑‡πà‡∏≠ 12 features ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô
    feature_cols = list(df.columns)
    
    # 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á (‡∏à‡∏≤‡∏Å 'close' ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà scale)
    future_price = df['close'].shift(-lookahead_bars)
    current_price = df['close']
    df['pct_change'] = (future_price - current_price) / current_price

    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á Labels
    def labeler(pct):
        if pct > hold_threshold_pct:
            return 1 # BUY
        elif pct < -hold_threshold_pct:
            return 2 # SELL
        else:
            return 0 # HOLD
    df['Target'] = df['pct_change'].apply(labeler)
    
    # 5. ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß NaN (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å)
    df.dropna(inplace=True) 
    
    # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á Sequences
    for i in range(len(df) - sequence_length):
        # üõë [FIX] ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 12 features (‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠)
        X.append(df.iloc[i:i+sequence_length][feature_cols].values) 
        
        # üõë [FIX] y ‡∏Ñ‡∏∑‡∏≠ Target ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πà‡∏á "‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô" (i + sequence_length - 1)
        y.append(df.iloc[i+sequence_length-1]['Target']) 

    return np.array(X), np.array(y)

# üõë C. SCALING (‡πÉ‡∏ä‡πâ MinMaxScaler) üõë
def scale_features(train_df, test_df=None, scaler=None):
    """Scales data using MinMaxScaler and returns the scaler."""
    if scaler is None:
        scaler = MinMaxScaler(feature_range=(0, 1))
        train_scaled = scaler.fit_transform(train_df)
    else:
        # ‡πÉ‡∏ä‡πâ Scaler ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ transform
        train_scaled = scaler.transform(train_df)
    
    train_scaled_df = pd.DataFrame(train_scaled, columns=train_df.columns)
    
    if test_df is not None:
        test_scaled = scaler.transform(test_df)
        test_scaled_df = pd.DataFrame(test_scaled, columns=test_df.columns)
        return scaler, train_scaled_df, test_scaled_df
        
    return scaler, train_scaled_df, None

# def collect_and_scale_data(days=180):
#     """
#     ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 3 Timeframes ‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
#     """
#     print("Fetching M5 data...")
#     df_m5_raw = get_xauusd_data(days, mt5.TIMEFRAME_M5)
#     print("Fetching M30 data...")
#     df_m30_raw = get_other_timeframe_data(days, mt5.TIMEFRAME_M30)
#     print("Fetching H1 data...")
#     df_h1_raw = get_other_timeframe_data(days, mt5.TIMEFRAME_H1)
    
#     if df_m5_raw.empty or df_m30_raw.empty or df_h1_raw.empty:
#         print("Data collection failed for one or more timeframes.")
#         return None, None, None, None, None

#     # ‡∏™‡πà‡∏á DF ‡∏ó‡∏±‡πâ‡∏á 3 ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
#     df_features = add_technical_indicators(df_m5_raw, df_m30_raw, df_h1_raw)

#     if df_features.empty:
#         print("Failed to create features or data was insufficient.")
#         return None, None, None, None, None

#     # Split: 80% train, 20% test
#     train_size = int(len(df_features) * 0.8)
#     train_df = df_features.iloc[:train_size]
#     test_df = df_features.iloc[train_size:]
    
#     # Scale: Fit only on training data
#     scaler, train_scaled_df, test_scaled_df = scale_features(train_df, test_df)

#     print(f"Total features scaled: {train_scaled_df.shape[1]}") 
    
#     return scaler, train_scaled_df, test_scaled_df, train_df, test_df

# ==============================================================================
# PART 2: MODEL ARCHITECTURE AND TRAINING
# ==============================================================================

# üõë D. BUILD GRU MODEL (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô) üõë
# üõë D. BUILD GRU MODEL (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô) üõë
def build_gru_model(input_shape):
    """
    [Simplified Model] ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏π‡πâ‡∏Å‡∏±‡∏ö Overfitting
    """
    print(f"Building 3-Class (Simplified) model with Input Shape: {input_shape}")
    
    model = Sequential([
        Input(shape=input_shape), 
        
        # ‚¨áÔ∏è ‡∏•‡∏î Units ‡∏à‡∏≤‡∏Å 128 -> 64
        # ‚¨áÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° Dropout ‡∏à‡∏≤‡∏Å 0.3 -> 0.4
        GRU(units=64, return_sequences=True, activation='tanh', kernel_regularizer=l2(0.001)),
        Dropout(0.4), 
        
        # ‚¨áÔ∏è ‡∏•‡∏î Units ‡∏à‡∏≤‡∏Å 64 -> 32
        # ‚¨áÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° Dropout ‡∏à‡∏≤‡∏Å 0.3 -> 0.4
        GRU(units=32, return_sequences=False, activation='tanh', kernel_regularizer=l2(0.001)),
        Dropout(0.4),
        
        # (Output Layer ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        Dense(units=3, activation='softmax')
    ])
    
    optimizer = Adam(learning_rate=0.001)
    
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model

# ... (‡∏™‡πà‡∏ß‡∏ô backtest_and_evaluate_model ‡πÅ‡∏•‡∏∞ train_rnn_model_main ‡πÄ‡∏î‡∏¥‡∏°) ...
# =============================================================================
# PART 3: TRAINING MAIN FUNCTION (Keep Existing Logic)
# =============================================================================

# def train_rnn_model_main(
#     sequence_length=100, 
#     lookahead_bars=1,
#     epochs=50, 
#     batch_size=32, 
#     days=180
#     # timeframe ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏î‡∏∂‡∏á 3 TFs
# ):
#     """
#     Main function to run the full training process.
#     """
#     print(f"Starting model training for XAUUSD (Multi-Timeframe)...")

#     # 1. Connect and Collect Data
#     if not initialize_mt5():
#         print("Cannot connect to MT5.")
#         return None
    
#     # üõë ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á timeframe
#     scaler, train_scaled_df, test_scaled_df, train_df, test_df = \
#         collect_and_scale_data(days=days)
    
#     # ... (‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô train_rnn_model_main ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î) ...
#     # (‡πÄ‡∏ä‡πà‡∏ô create_sequences_and_labels, Handle Class Imbalance,
#     #  Build Model, Callbacks, Train Model, Save model/scaler, Upload)
    
#     if train_scaled_df is None or test_scaled_df is None:
#         print("Data collection failed or returned empty dataframes.")
#         mt5.shutdown()
#         return None
    
#     print(f"Train/Test split: {len(train_scaled_df)} / {len(test_scaled_df)} bars.")

#     # 2. Create Sequences and Labels (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     X_train, y_train = create_sequences_and_labels(
#         train_scaled_df, 
#         sequence_length=sequence_length,
#         lookahead_bars=lookahead_bars
#     )
#     X_test, y_test = create_sequences_and_labels(
#         test_scaled_df, 
#         sequence_length=sequence_length,
#         lookahead_bars=lookahead_bars
#     )
    
#     if X_train.shape[0] == 0:
#          print("Not enough data to create training sequences.")
#          mt5.shutdown()
#          return None

#     # (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Input Shape ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
#     print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
#     print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")
    
#     # 3. Handle Class Imbalance (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     unique_classes, counts = np.unique(y_train, return_counts=True)
#     if len(unique_classes) > 1:
#         class_weights = class_weight.compute_class_weight(
#             class_weight='balanced', 
#             classes=unique_classes, 
#             y=y_train
#         )
#         class_weight_dict = dict(zip(unique_classes, class_weights))
#         print(f"Class Weights: {class_weight_dict}")
#     else:
#         class_weight_dict = {unique_classes[0]: 1.0}

#     # 4. Build Model (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏£‡∏±‡∏ö input_shape ‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
#     input_shape = (sequence_length, X_train.shape[2]) # X_train.shape[2] ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 11
#     model = build_gru_model(input_shape)
#     print("Model built and compiled.")

#     # 5. Define Callbacks (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     model_checkpoint_callback = ModelCheckpoint(
#         filepath='models/gru_bot_best_M5.h5', 
#         monitor='val_accuracy', 
#         save_best_only=True, 
#         mode='max', 
#         verbose=1
#     )
#     early_stopping_callback = EarlyStopping(
#         monitor='val_loss', 
#         patience=10, 
#         mode='min', 
#         restore_best_weights=True,
#         verbose=1
#     )

#     # 6. Train Model (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     print("Starting training...")
#     history = model.fit(
#         X_train, y_train,
#         epochs=epochs,
#         batch_size=batch_size,
#         validation_data=(X_test, y_test),
#         callbacks=[model_checkpoint_callback, early_stopping_callback],
#         class_weight=class_weight_dict if len(unique_classes) > 1 else None,
#         verbose=2
#     )
#     print("Training finished.")

#     # 7. Load best model (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     best_model = load_model('models/gru_bot_best_M5.h5')
    
#     # 8. Save final model and scaler (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     os.makedirs('models', exist_ok=True)
#     with open('models/scaler.pkl', 'wb') as f:
#         pickle.dump(scaler, f)
#     print("\nTraining completed. Model and Scaler saved locally.")  

#     # 9. üöÄ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡∏∂‡πâ‡∏ô GitHub (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     upload_to_github("models/gru_bot_best_M5.h5", "models/gru_bot_best_M5.h5")
#     upload_to_github("models/scaler.pkl", "models/scaler.pkl")

#     mt5.shutdown()
#     return best_model

# ==============================================================================
# MAIN EXECUTION
# ==============================================================================
if __name__ == "__main__":
    
    # üö® Configuration based on typical RNN setup
    FINAL_SEQUENCE_LENGTH = 100 # Lookback bars for prediction
    FINAL_LOOKAHEAD_BARS = 1   # Predict 1 bar ahead
    
    # train_rnn_model_main(
    #     sequence_length=FINAL_SEQUENCE_LENGTH,
    #     lookahead_bars=FINAL_LOOKAHEAD_BARS,
    #     epochs=50,
    #     batch_size=32,
    #     days=180 # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 180 ‡∏ß‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å
    # )

# ... (End of model.py)