#import MetaTrader5 as mt5
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pytz
import talib
import pickle
import os
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import class_weight
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import GRU, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf
#from github import Github
import talib

# ==========================
# CONFIG ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GitHub
# ==========================
# GITHUB_TOKEN = "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"  # <- ‡πÉ‡∏™‡πà Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
# REPO_NAME = "bookhub10/models"                  # <- ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠ repo ‡πÄ‡∏ä‡πà‡∏ô oakjkp/gru-bot-model
# GITHUB_BRANCH = "main"                        # ‡∏™‡∏≤‡∏Ç‡∏≤

# ==========================
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î GitHub
# ==========================
# def upload_to_github(local_path, repo_path, message="Upload model to GitHub"):
#     """Upload local file to GitHub repo"""
#     g = Github(GITHUB_TOKEN)
#     repo = g.get_repo(REPO_NAME)
#     with open(local_path, "rb") as f:
#         content = f.read()
#     try:
#         repo.create_file(repo_path, message, content, branch=GITHUB_BRANCH)
#         print(f"Uploaded {local_path} to GitHub as {repo_path}")
#     except Exception as e:
#         # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ update ‡πÅ‡∏ó‡∏ô
#         try:
#             file = repo.get_contents(repo_path, ref=GITHUB_BRANCH)
#             repo.update_file(file.path, message, content, file.sha, branch=GITHUB_BRANCH)
#             print(f"Updated {local_path} on GitHub as {repo_path}")
#         except Exception as e2:
#             print(f"Error uploading {local_path}: {e2}")

# ==============================================================================
# PART 1: DATA COLLECTION AND PREPROCESSING
# ==============================================================================

# def initialize_mt5(login=0, password="", server=""):
#     """Initialize connection to MetaTrader 5"""
#     if not mt5.initialize():
#         print("initialize() failed, retrying...")
#         if not mt5.initialize():
#             print("initialize() failed finally.")
#             return False
    
#     if login != 0:
#         if not mt5.login(login, password=password, server=server):
#             print("Failed to connect to trade account", login)
#             mt5.shutdown()
#             return False
#     return True

# def get_xauusd_data(days=180, timeframe=mt5.TIMEFRAME_M5):
#     """Get XAUUSD data FOR M5 (Base)"""
#     # (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£)
#     timezone = pytz.timezone("Etc/UTC")
#     time_from = datetime.now(timezone) - timedelta(days=days)
#     rates = mt5.copy_rates_from(
#         "XAUUSD", 
#         timeframe, 
#         time_from, 
#         30000 
#     )
#     if rates is None or len(rates) == 0:
#         print("Failed to get XAUUSD data.")
#         return pd.DataFrame()
#     df = pd.DataFrame(rates)
#     df['time'] = pd.to_datetime(df['time'], unit='s')
#     df.set_index('time', inplace=True)
#     df.drop(columns=['spread', 'real_volume'], inplace=True)
#     return df

# def get_other_timeframe_data(days, timeframe):
#     """üÜï ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TF ‡∏≠‡∏∑‡πà‡∏ô"""
#     timezone = pytz.timezone("Etc/UTC")
#     time_from = datetime.now(timezone) - timedelta(days=days)
#     rates = mt5.copy_rates_from("XAUUSD", timeframe, time_from, 30000)
#     if rates is None or len(rates) == 0:
#         return pd.DataFrame()
#     df = pd.DataFrame(rates)
#     df['time'] = pd.to_datetime(df['time'], unit='s')
#     df.set_index('time', inplace=True)
#     df = df[['close']] # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Ñ‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î
#     return df

# üõë A. ADD TECHNICAL INDICATORS (‡∏ï‡∏≤‡∏° Obot_model) üõë
def add_technical_indicators(df_m5, df_m30, df_h1):
    """
    ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å M5, M30, ‡πÅ‡∏•‡∏∞ H1 (using TA-Lib only)
    """
    
    # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå M5 (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ===
    df_m5 = df_m5.copy()
    close_prices_m5 = df_m5['close'].values.astype(np.float64)
    
    df_m5['SMA_10'] = talib.SMA(close_prices_m5, timeperiod=10)
    df_m5['SMA_50'] = talib.SMA(close_prices_m5, timeperiod=50)
    df_m5['Momentum_1'] = talib.MOM(close_prices_m5, timeperiod=1)
    df_m5['High_Low'] = df_m5['high'] - df_m5['low']

    # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå Multi-Timeframe (using TA-Lib) ===
    
    # 2.1: M30 RSI (RSI 14 ‡∏ö‡∏ô Timeframe M30)
    # üÜï ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ talib.RSI ‡πÅ‡∏ó‡∏ô pandas_ta
    close_prices_m30 = df_m30['close'].values.astype(np.float64)
    df_m30['M30_RSI'] = talib.RSI(close_prices_m30, timeperiod=14)
    
    # 2.2: H1 MA Trend (MA 200 ‡∏ö‡∏ô Timeframe H1)
    # üÜï ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ talib.SMA ‡πÅ‡∏ó‡∏ô pandas_ta
    close_prices_h1 = df_h1['close'].values.astype(np.float64)
    df_h1['H1_MA_200'] = talib.SMA(close_prices_h1, timeperiod=200)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå Trend: 1 ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡πÄ‡∏™‡πâ‡∏ô, 0 ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ï‡πâ‡πÄ‡∏™‡πâ‡∏ô (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
    df_h1['H1_MA_Trend'] = np.where(df_h1['close'] > df_h1['H1_MA_200'], 1, 0)
    
    # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á" (Alignment) ===
    # (‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° 100% ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ä‡πâ pandas ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
    
    print("Aligning M30 features to M5 timeline...")
    df_combined = pd.merge_asof(
        df_m5.sort_index(), 
        df_m30[['M30_RSI']].sort_index(), 
        left_index=True, 
        right_index=True, 
        direction='backward'
    )
    
    print("Aligning H1 features to M5 timeline...")
    df_final = pd.merge_asof(
        df_combined.sort_index(),
        df_h1[['H1_MA_Trend']].sort_index(),
        left_index=True,
        right_index=True,
        direction='backward'
    )
    
    # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° 100%) ===
    df_final.dropna(inplace=True)
    df_final.reset_index(drop=True, inplace=True)

    feature_cols = [
        'open', 'high', 'low', 'close', 'tick_volume', 
        'SMA_10', 'SMA_50', 'Momentum_1', 'High_Low',
        'M30_RSI', 'H1_MA_Trend'  # <-- ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà 2 ‡∏ï‡∏±‡∏ß
    ]
    
    final_cols = [col for col in feature_cols if col in df_final.columns]
    if len(final_cols) != len(feature_cols):
        print(f"Warning: Missing columns! Expected {len(feature_cols)}, found {len(final_cols)}")
        
    df_final = df_final[final_cols].copy()
    
    print(f"Total features created: {len(df_final.columns)}")
    return df_final

# üõë B. CREATE SEQUENCES AND LABELS (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î) üõë
def create_sequences_and_labels(df, sequence_length=100, lookahead_bars=1):
    """
    Prepares data into sequences (X) and next-bar direction labels (Y).
    Labeling: 1 for Buy (price goes up), 0 for Sell/Hold (price stays same or drops).
    """
    X, y = [], []
    df_values = df.values
    
    # ‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Label: Price Up vs. Price Down/Same
    # üö® NOTE: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ lookahead_bars ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï 1 ‡πÅ‡∏ó‡πà‡∏á
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Label
    # y[i] = 1 ‡∏ñ‡πâ‡∏≤ Close[i + lookahead_bars] > Close[i]
    # y[i] = 0 ‡∏ñ‡πâ‡∏≤ Close[i + lookahead_bars] <= Close[i]
    
    df['Target'] = np.where(df['close'].shift(-lookahead_bars) > df['close'], 1, 0)
    
    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ Target
    df.dropna(subset=['Target'], inplace=True)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Sequences
    for i in range(len(df) - sequence_length):
        X.append(df.iloc[i:i+sequence_length][df.columns[:-1]].values) # Features
        y.append(df.iloc[i+sequence_length-1]['Target']) # Target ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ó‡πà‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Sequence

    return np.array(X), np.array(y)


# üõë C. SCALING (‡πÉ‡∏ä‡πâ MinMaxScaler) üõë
def scale_features(train_df, test_df=None, scaler=None):
    """Scales data using MinMaxScaler and returns the scaler."""
    if scaler is None:
        scaler = MinMaxScaler(feature_range=(0, 1))
        train_scaled = scaler.fit_transform(train_df)
    else:
        # ‡πÉ‡∏ä‡πâ Scaler ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ transform
        train_scaled = scaler.transform(train_df)
    
    train_scaled_df = pd.DataFrame(train_scaled, columns=train_df.columns)
    
    if test_df is not None:
        test_scaled = scaler.transform(test_df)
        test_scaled_df = pd.DataFrame(test_scaled, columns=test_df.columns)
        return scaler, train_scaled_df, test_scaled_df
        
    return scaler, train_scaled_df, None

# def collect_and_scale_data(days=180):
#     """
#     ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 3 Timeframes ‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
#     """
#     print("Fetching M5 data...")
#     df_m5_raw = get_xauusd_data(days, mt5.TIMEFRAME_M5)
#     print("Fetching M30 data...")
#     df_m30_raw = get_other_timeframe_data(days, mt5.TIMEFRAME_M30)
#     print("Fetching H1 data...")
#     df_h1_raw = get_other_timeframe_data(days, mt5.TIMEFRAME_H1)
    
#     if df_m5_raw.empty or df_m30_raw.empty or df_h1_raw.empty:
#         print("Data collection failed for one or more timeframes.")
#         return None, None, None, None, None

#     # ‡∏™‡πà‡∏á DF ‡∏ó‡∏±‡πâ‡∏á 3 ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
#     df_features = add_technical_indicators(df_m5_raw, df_m30_raw, df_h1_raw)

#     if df_features.empty:
#         print("Failed to create features or data was insufficient.")
#         return None, None, None, None, None

#     # Split: 80% train, 20% test
#     train_size = int(len(df_features) * 0.8)
#     train_df = df_features.iloc[:train_size]
#     test_df = df_features.iloc[train_size:]
    
#     # Scale: Fit only on training data
#     scaler, train_scaled_df, test_scaled_df = scale_features(train_df, test_df)

#     print(f"Total features scaled: {train_scaled_df.shape[1]}") 
    
#     return scaler, train_scaled_df, test_scaled_df, train_df, test_df

# ==============================================================================
# PART 2: MODEL ARCHITECTURE AND TRAINING
# ==============================================================================

# üõë D. BUILD GRU MODEL (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô) üõë
# üõë D. BUILD GRU MODEL (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô) üõë
def build_gru_model(input_shape):
    """
    Defines the GRU-RNN model architecture.
     input_shape ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô (sequence_length, 11)
    """
    print(f"Building model with Input Shape: {input_shape}") # <-- ‡πÄ‡∏û‡∏¥‡πà‡∏° log
    
    model = Sequential([
        Input(shape=input_shape), # <-- input_shape ‡πÉ‡∏´‡∏°‡πà
        # 1st GRU Layer
        GRU(units=128, return_sequences=True, activation='tanh', kernel_regularizer=l2(0.001)),
        Dropout(0.3),
        # 2nd GRU Layer
        GRU(units=64, return_sequences=False, activation='tanh', kernel_regularizer=l2(0.001)),
        Dropout(0.3),
        # Output Layer
        Dense(units=1, activation='sigmoid')
    ])
    
    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    
    return model

# ... (‡∏™‡πà‡∏ß‡∏ô backtest_and_evaluate_model ‡πÅ‡∏•‡∏∞ train_rnn_model_main ‡πÄ‡∏î‡∏¥‡∏°) ...
# =============================================================================
# PART 3: TRAINING MAIN FUNCTION (Keep Existing Logic)
# =============================================================================

# def train_rnn_model_main(
#     sequence_length=100, 
#     lookahead_bars=1,
#     epochs=50, 
#     batch_size=32, 
#     days=180
#     # timeframe ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏î‡∏∂‡∏á 3 TFs
# ):
#     """
#     Main function to run the full training process.
#     """
#     print(f"Starting model training for XAUUSD (Multi-Timeframe)...")

#     # 1. Connect and Collect Data
#     if not initialize_mt5():
#         print("Cannot connect to MT5.")
#         return None
    
#     # üõë ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á timeframe
#     scaler, train_scaled_df, test_scaled_df, train_df, test_df = \
#         collect_and_scale_data(days=days)
    
#     # ... (‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô train_rnn_model_main ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î) ...
#     # (‡πÄ‡∏ä‡πà‡∏ô create_sequences_and_labels, Handle Class Imbalance,
#     #  Build Model, Callbacks, Train Model, Save model/scaler, Upload)
    
#     if train_scaled_df is None or test_scaled_df is None:
#         print("Data collection failed or returned empty dataframes.")
#         mt5.shutdown()
#         return None
    
#     print(f"Train/Test split: {len(train_scaled_df)} / {len(test_scaled_df)} bars.")

#     # 2. Create Sequences and Labels (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     X_train, y_train = create_sequences_and_labels(
#         train_scaled_df, 
#         sequence_length=sequence_length,
#         lookahead_bars=lookahead_bars
#     )
#     X_test, y_test = create_sequences_and_labels(
#         test_scaled_df, 
#         sequence_length=sequence_length,
#         lookahead_bars=lookahead_bars
#     )
    
#     if X_train.shape[0] == 0:
#          print("Not enough data to create training sequences.")
#          mt5.shutdown()
#          return None

#     # (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Input Shape ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
#     print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
#     print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")
    
#     # 3. Handle Class Imbalance (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     unique_classes, counts = np.unique(y_train, return_counts=True)
#     if len(unique_classes) > 1:
#         class_weights = class_weight.compute_class_weight(
#             class_weight='balanced', 
#             classes=unique_classes, 
#             y=y_train
#         )
#         class_weight_dict = dict(zip(unique_classes, class_weights))
#         print(f"Class Weights: {class_weight_dict}")
#     else:
#         class_weight_dict = {unique_classes[0]: 1.0}

#     # 4. Build Model (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏£‡∏±‡∏ö input_shape ‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
#     input_shape = (sequence_length, X_train.shape[2]) # X_train.shape[2] ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 11
#     model = build_gru_model(input_shape)
#     print("Model built and compiled.")

#     # 5. Define Callbacks (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     model_checkpoint_callback = ModelCheckpoint(
#         filepath='models/gru_bot_best_M5.h5', 
#         monitor='val_accuracy', 
#         save_best_only=True, 
#         mode='max', 
#         verbose=1
#     )
#     early_stopping_callback = EarlyStopping(
#         monitor='val_loss', 
#         patience=10, 
#         mode='min', 
#         restore_best_weights=True,
#         verbose=1
#     )

#     # 6. Train Model (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     print("Starting training...")
#     history = model.fit(
#         X_train, y_train,
#         epochs=epochs,
#         batch_size=batch_size,
#         validation_data=(X_test, y_test),
#         callbacks=[model_checkpoint_callback, early_stopping_callback],
#         class_weight=class_weight_dict if len(unique_classes) > 1 else None,
#         verbose=2
#     )
#     print("Training finished.")

#     # 7. Load best model (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     best_model = load_model('models/gru_bot_best_M5.h5')
    
#     # 8. Save final model and scaler (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     os.makedirs('models', exist_ok=True)
#     with open('models/scaler.pkl', 'wb') as f:
#         pickle.dump(scaler, f)
#     print("\nTraining completed. Model and Scaler saved locally.")  

#     # 9. üöÄ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡∏∂‡πâ‡∏ô GitHub (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
#     upload_to_github("models/gru_bot_best_M5.h5", "models/gru_bot_best_M5.h5")
#     upload_to_github("models/scaler.pkl", "models/scaler.pkl")

#     mt5.shutdown()
#     return best_model

# ==============================================================================
# MAIN EXECUTION
# ==============================================================================
if __name__ == "__main__":
    
    # üö® Configuration based on typical RNN setup
    FINAL_SEQUENCE_LENGTH = 100 # Lookback bars for prediction
    FINAL_LOOKAHEAD_BARS = 1   # Predict 1 bar ahead
    
    # train_rnn_model_main(
    #     sequence_length=FINAL_SEQUENCE_LENGTH,
    #     lookahead_bars=FINAL_LOOKAHEAD_BARS,
    #     epochs=50,
    #     batch_size=32,
    #     days=180 # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 180 ‡∏ß‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å
    # )

# ... (End of model.py)